/*  Okapi YAML Parser  Copyright (C) 2015 by the Okapi Framework contributors  -----------------------------------------------------------------------------  Licensed under the Apache License, Version 2.0 (the "License");  you may not use this file except in compliance with the License.  You may obtain a copy of the License at  http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an "AS IS" BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License.  Sections are taken from:   https://github.com/RobertFischer/Yaml-parser  http://creativecommons.org/publicdomain/zero/1.0/  To the extent possible under law, Robert Fischer has waived all copyright  and related or neighboring rights to Yaml Parser.   and   https://github.com/mwnorman/YamlParser/blob/master/src/org/mwnorman/Yaml/YamlParser.jj  ISCL http://www.opensource.org/licenses/isc-license.txt  Copyright (c) 2011-2013 Mike Norman  Permission to use, copy, modify, and/or distribute this software for any  purpose with or without fee is hereby granted, provided that the above  copyright notice and this permission notice appear in all copies.   THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES  WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF  MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY  SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER  RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT,  NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE  USE OR PERFORMANCE OF THIS SOFTWARE.*/options{  GENERATE_ANNOTATIONS=true;   GENERATE_CHAINED_EXCEPTION=true;   SUPPORT_CLASS_VISIBILITY_PUBLIC=true;   COMMON_TOKEN_ACTION=true;   GENERATE_GENERICS=true;   GENERATE_STRING_BUILDER=true;   UNICODE_INPUT=true;   JAVA_UNICODE_ESCAPE=true;   JDK_VERSION="1.7";   TOKEN_EXTENDS="BaseToken";     CHOICE_AMBIGUITY_CHECK = 2;  OTHER_AMBIGUITY_CHECK = 1;  ERROR_REPORTING = true;  IGNORE_CASE = false;  SANITY_CHECK = true;  STATIC = false;  KEEP_LINE_COLUMN = true;  JAVA_TEMPLATE_TYPE="modern";  //DEBUG_PARSER = true;  //DEBUG_LOOKAHEAD = true;  //DEBUG_TOKEN_MANAGER = true;}PARSER_BEGIN(YamlParser)package net.sf.okapi.filters.yaml.parser;import java.io.StringReader;import java.util.ArrayList;import java.util.List;import net.sf.okapi.common.Util;/**
//* Basic YAML parser.
*/public class YamlParser{  private IYamlHandler handler = null;    public void setHandler(IYamlHandler handler)  {    this.handler = handler;  }    public void reset() {  	 token_source.reset();  }  public void parse() throws ParseException  {    token_source.initialize(handler);    handler.handleStart();        yaml_stream();    handler.handleEnd();      }  		public String ignorableTextBefore(Token t, boolean firstTime) {		StringBuilder s = new StringBuilder(); 		List<Token> specialTokenList = token_source.getSpecialTokensBefore(t);	    if (specialTokenList != null) { 	      for (Token st : specialTokenList) {	      	if (firstTime) {	      		switch(st.kind) {	      			case COMMENT_TEXT:	      				handler.handleComment(st.image, true);	      			break;	      			case EOL:	      				      				      			case SPACE:	      				handler.handleWhitespace(st.image, true);	      			break;	      			default:	      				handler.handleOther(t.image);	      			break;	      		}	      	} else {	      		s.append(st.image);	      	}	      }	    }	    return s.toString();	}}PARSER_END(YamlParser)TOKEN_MGR_DECLS :{   // should be enough even for very large files	static final int MAX_INDENTS = 1000;	// make first column 0 so that DASH and keys at column 1 produce an "indentation"	static final int FIRST_COLUMN = 0;		/** The stack of indent levels (column numbers) */	int[] indentStack = new int[MAX_INDENTS];	int[] indentTokenTypes = new int[MAX_INDENTS];	// current indent pointer	int sp = 0;	int pendingDedents = 0;	Token lastToken;    int flowSequenceNesting = 0;	int flowMappingeNesting = 0;	boolean linestart = false;	IYamlHandler handler = null;			// special tokens collected at the end of an indented block	List<Token> endIndentedSpecial;		void reset() {		indentStack = new int[MAX_INDENTS];		indentTokenTypes = new int[MAX_INDENTS];		// current indent pointer		sp = 0;		pendingDedents = 0;		lastToken = null;	    flowSequenceNesting = 0;		flowMappingeNesting = 0;		linestart = false;				// special tokens collected at the end of an indented block		endIndentedSpecial = null;				indentStack[sp] = FIRST_COLUMN;		indentTokenTypes[sp] = -1; 		curLexState = DEFAULT;	}		void CommonTokenAction(Token currentToken) {		int tokenSize = currentToken.image.length();		int currentColumn = currentToken.beginColumn;			    // turn off indent logic for these tokens		if (ignoreIndent(currentToken.kind)) {			return;		}								// handle pending dedents on each pass until done		if (pendingDedents > 0) {			handlePendingDedent(currentToken);			return;		}		// if this is the resent token then just return it		if (currentToken.kind != EOF && lastToken != null 				&& lastToken.beginColumn == currentToken.beginColumn				&& lastToken.beginLine == currentToken.beginLine) {			return;		}		int lastLine = (lastToken == null) ? 0 : lastToken.endLine;		int currentLine = currentToken.beginLine;		// We are on the same line, so no indent/dedent here, so just return.		// except for KEY or DASH tokens as they can be on the same line as a DASH but require		// an "indent" sent to the parser		if (currentToken.kind != EOF && currentLine == lastLine && 			!isBlockKey(currentToken.kind) && currentToken.kind != DASH) {			lastToken = currentToken;			return;		}		// check for dedents		if (unwindIndent(currentToken))	{			lastToken = currentToken;		  				return;		}		// handle block indentation tokens		if (currentToken.kind == DASH || isBlockKey(currentToken.kind)) {		  				if (peekIndent() < currentColumn) {				// adjust indent of key was labled with tag - use the tag indent				if (lastTokenWasTag(lastToken) && lastToken.beginLine == currentToken.beginLine) {					push(lastToken.beginColumn, currentToken.kind);				} else {					push(currentColumn, currentToken.kind);				}													if (currentToken.kind == DASH && !isIndentlessBlock(currentToken)) {					backup(tokenSize);									currentToken.kind = BLOCK_SEQUENCE_START;					currentToken.image = "BLOCK_SEQUENCE_START";							} else if (isBlockKey(currentToken.kind)) {				  	backup(tokenSize);									currentToken.kind = BLOCK_MAPPING_START;					currentToken.image = "BLOCK_MAPPING_START";				}			  			} 					} else if (peekType() == DASH && isBlockKey(currentToken.kind) && 			currentLine == lastLine) {			// special case where a key follows a DASH on the same line			push(currentColumn, currentToken.kind);			backup(tokenSize);							currentToken.kind = BLOCK_MAPPING_START;			currentToken.image = "BLOCK_MAPPING_START";					} else if (peekType() == DASH && currentToken.kind == DASH && 			currentLine == lastLine) {			// special case where a DASH follows a DASH on the same line			backup(tokenSize);							currentToken.kind = BLOCK_SEQUENCE_START;			currentToken.image = "BLOCK_SEQUENCE_START";			}								lastToken = currentToken;	}	public void initialize(IYamlHandler h) {		reset();		handler = h;	}		public boolean lastTokenWasTag(Token t) {		if (t == null) return false;		if (t.kind == TAG) return true;		return false;	}	private boolean isIndentlessBlock(Token indentCandidate)	{	  	if (indentCandidate.kind == DASH)	  	{			if(peekIndent() == FIRST_COLUMN || 				(isBlockKey(peekType()) && peekIndent() == indentCandidate.beginColumn))			{				return true;			}	  	}		return false;	}	public boolean isBlockKey(int kind) {	  switch(curLexState) {	    case FLOW:	   	    	return false;	  }	  	        switch(kind) {        case PLAIN_KEY:        case SINGLE_QUOTED_KEY:        case DOUBLE_QUOTED_KEY:                       return true;      }      return false;	}    public boolean ignoreIndent(int kind) {    	switch(curLexState) {	    	case FLOW:	   	  	 		return true;	  	}	        switch(kind) {        case PLAIN_SCALAR_INDENTED:        case LITERAL_FOLDED_INDENTED:        case NULL_SCALAR:        case SINGLE_QUOTED_EMPTY:        case DOUBLE_QUOTED_EMPTY:        case SINGLE_QUOTED_FIRST:        case DOUBLE_QUOTED_FIRST:        case STRING_DOUBLE_BODY_SECOND:        case STRING_SINGLE_BODY_SECOND:        case QUOTE_SINGLE_END:        case QUOTE_DOUBLE_END:        case CONTINUATION_START:        case CONTINUATION_END:            return true;      }      return false;    }		public boolean canUnindent(Token t)	{			  switch(curLexState) {	    case FLOW:	   	    	return false;	  }	  	  switch(t.kind) {	    case FLOW_SEQUENCE_END:	    case FLOW_MAPPING_END:	    	return false;	  }	  return true;	}		private void backup(int n) { input_stream.backup(n); }	private boolean unwindIndent(Token t)	{	  	if (!canUnindent(t)) {	  		return false;	  	}	  		  	int beginColumn = t.beginColumn;	  	// unwind indent all the way when we see these 	  	if (t.kind == EOF || t.kind == DOCUMENT_START || t.kind == DOCUMENT_END) {	  		beginColumn = FIRST_COLUMN;	  	}		if (peekIndent() > beginColumn) {			// how far back did we dedent?			int prevIndex = findPreviousIndent(beginColumn);			// generate DEDENTs for each indent level we backed up over			for (int d = sp - 1; d >= prevIndex; d--) {				pendingDedents++;			}			// take care of one unindent now, the remaining on the next token			handlePendingDedent(t);			return true;		}		return false;	} 		private boolean isIndentedScalarFinished(Token t) {		// not a match as the indent is > than parent, put the token back	    if (peekIndent() >= t.beginColumn) {	    	// make sure we handle comments or whitespace after this indented block	    	// but have to save until the other tokens are processed	    	endIndentedSpecial = getSpecialTokensBefore(t);	    	backup(t.image.length());	    	t.kind = NULL_SCALAR;			t.image = "";				return true;	    }	    	    return false; 	}		public void processIndentedBlockEnd() {    	// make sure we handled any preceding comments or whitespace    	if (endIndentedSpecial != null) {    		boolean first = true;	    	for(Token s : endIndentedSpecial) {	    		if (s.kind == COMMENT_TEXT) {	    			// handle case were the lexer eats an extra whitespace after plain scalar and before comment	    			// re-add the missing whitespace as we strip it from the scalar	    			handler.handleComment(first ? " "+s.image : s.image, true);	    		} else {	    			handler.handleWhitespace(s.image, true);	    		}	    		first = false;	    	}  	    	endIndentedSpecial = null;    	}	}  		private void handlePendingDedent(Token t) {		if (t.kind != FALSE_START) {	  	  		backup(t.image.length());	  	}		t.kind = BLOCK_END;		t.image = "BLOCK_END";		// never go below zero in case this is a EOF		if (pendingDedents > 0) --pendingDedents;		pop();	}		private void push(int indent, int type) {		if (sp >= MAX_INDENTS) {			throw new IllegalStateException("Indent Error");		}		sp++;		indentStack[sp] = indent;		indentTokenTypes[sp] = type;	}	private int pop() {		if (sp < 0) {			throw new IllegalStateException("Unindent error");		}		int top = indentStack[sp];		sp--;		return top;	}	public int peekIndent() {		return indentStack[sp];	}	public int peekType() {		return indentTokenTypes[sp];	}		public boolean insideFlow() {		if (flowSequenceNesting==0 && flowMappingeNesting==0)			return false;		else 			return true;	}		/** Return the index on stack of previous indent level == i else -1 */	private int findPreviousIndent(int i) {		for (int j = sp - 1; j >= 0; j--) {			if (indentStack[j] == i) {				return j;			}		}		return -1;	}		private void ignorableWhiteSpace(Token t) {		if (curLexState == DEFAULT || curLexState == FLOW) {			handler.handleWhitespace(t.image, false);		} 		// else we are inside some scalar with newlines let the scalar code		// handle it	}		private void ignorableComment(Token t) {		if (curLexState == DEFAULT || curLexState == FLOW) {			handler.handleComment(t.image, false);		} 		// else we are inside some scalar with newlines let the scalar code		// handle it	}		public List<Token> getSpecialTokensBefore(Token t)  	{	    if (t.specialToken == null) return null;	    List < Token > specialTokensList = new ArrayList <Token> ();	    // The above statement determines that there are no special tokens	    // and returns control to the caller.	    Token tmp_t = t.specialToken;	    while (tmp_t.specialToken != null) tmp_t = tmp_t.specialToken;	    // The above line walks back the special token chain until it	    // reaches the first special token after the previous regular	    // token.	    while (tmp_t != null)	    {	      specialTokensList.add(tmp_t);	      tmp_t = tmp_t.next;	    }	    // The above loop now walks the special token chain in the forward	    // direction	    return specialTokensList;  	}}//# A '#' starts a comment until end of line of EOF<*> SPECIAL_TOKEN :{  <COMMENT_TEXT: "#" (~["\n","\r"])* (<EOL>)?>  {  	ignorableComment(matchedToken);  }}<UNREACHABLE> TOKEN :{  <BLOCK_SEQUENCE_START : "BLOCK_SEQUENCE_START">| <BLOCK_MAPPING_START : "BLOCK_MAPPING_START">| <BLOCK_END: "BLOCK_END" >| <NULL_SCALAR: "NULL_SCALAR" >| <FALSE_START: "FALSE_START" >}<DEFAULT, FLOW> TOKEN :{  <#WORD_CHAR: ["0"-"9", "a"-"z", "A"-"Z", "_", "-"]>| <#URI_CHAR:  <WORD_CHAR>|["%", "#", ";", "/", "?", ":", "@", "&", "=", "+", "$", ",", ".", "!", "~", "*", "'", "(", ")", "[", "]"]>| <TAG: ("!"|"!!") ("<")? (<URI_CHAR>)* (">")?  ((<SPACE>)+ | ((<SPACE>)* <EOL>))>| <TAG_DIRECTIVE: "%TAG" (<SPACE>)+ (~["\r", "\n"])* <EOL>>| <VERSION_TAG: "%YAML" (<SPACE>)+ (["0"-"9", "."])+ <EOL>>| <ANCHOR: "&"(<WORD_CHAR>)+>| <ALIAS: (<SPACE>|<EOL>)* "*" (<FIRST_CHARS>)+>  }<DEFAULT, FLOW> TOKEN :{  <FLOW_SEQUENCE_START : "["> {flowSequenceNesting++;} : FLOW| <FLOW_MAPPING_START : "{"> {flowMappingeNesting++;} : FLOW}<FLOW> TOKEN :{  <FLOW_SEQUENCE_END : "]"> {flowSequenceNesting--; if (!insideFlow()) SwitchTo(DEFAULT);}| <FLOW_MAPPING_END : "}"> {flowMappingeNesting--;  if (!insideFlow()) SwitchTo(DEFAULT);}| <LIST_SEPERATOR: "," ((<SPACE>)* | ((<SPACE>)* <EOL>)) >}// let CommonToken chnage back to DEFAULT state<DEFAULT, DOCUMENT_START_END> TOKEN :{  <DOCUMENT_START: "---" ((<SPACE>)+ | ((<SPACE>)* <EOL>))>| <DOCUMENT_END: "..." (<SPACE>)* (<EOL>)?>}// trim whitespace in parser code if needed<DEFAULT, FLOW> TOKEN :{  <#HEXNUMBER: "x" (["0"-"9","a"-"f","A"-"F"])+>  | <#ESCAPES: ("r"|"n"|"f"|"\\"|"/"|"\""|"b"|"t"|"0"|"v"|"e"|"N"|"_"|"p")>   | <#QUOTE_DOUBLE: "\"" > | <#QUOTE_SINGLE: "'" > | <#STRING_SINGLE_BODY_FIRST: (~["'", "\r", "\n"] | "''")+>| <#STRING_DOUBLE_BODY_FIRST:    (      (~[ "\"", "\\", "\f", "\t", "\r", "\n"])    |      (        "\\"        (          <ESCAPES>          | <HEXNUMBER>        )      )    )+>| <SINGLE_QUOTED_EMPTY : "''" >| <DOUBLE_QUOTED_EMPTY : "\"\"" >| <SINGLE_QUOTED_FIRST : <QUOTE_SINGLE> <STRING_SINGLE_BODY_FIRST>> : SINGLEQUOTE| <DOUBLE_QUOTED_FIRST : <QUOTE_DOUBLE> <STRING_DOUBLE_BODY_FIRST>> : DOUBLEQUOTE}// these continuation tokens and whitespace are eaten<DOUBLEQUOTE> TOKEN :{  <CONTINUATION_END: "\\" (<SPACE>)* <EOL>>| <CONTINUATION_START: (<SPACE>)* "\\">}<DOUBLEQUOTE> TOKEN :{  // if newlines and indents we grab all the string begin/end whitespace but trim it later in the code  <STRING_DOUBLE_BODY_SECOND:    (      (~["\"", "\\", "\f", "\t", "\r", "\n"])    |      (        "\\"        (          <ESCAPES>          | <HEXNUMBER>        )      )    )+>| <QUOTE_DOUBLE_END: "\"" >{ 	if (insideFlow()) SwitchTo(FLOW); 	else SwitchTo(DEFAULT); } }<SINGLEQUOTE> TOKEN :{   // if newlines and indents we grab all the string begin/end whitespace but trim it later in the code  <STRING_SINGLE_BODY_SECOND: (~["'", "\r", "\n"] | "''")+>| <QUOTE_SINGLE_END: "'" > { 	if (insideFlow()) SwitchTo(FLOW); 	else SwitchTo(DEFAULT); } }TOKEN :{   // Key, plain and flow scalars can't start with these chars  <#FIRST_CHARS: ~[",", "[", "]", "{", "}", "#", "&", "*", "!", "|", ">", "'", "\"", "%", "@", "`", " ", "\r", "\n", "-", "?", ":"]   	|   	// any of these chars followed by a non space is ok   	(["-", "?", ":"] ~[" ", "\r", "\n"])> }TOKEN :{  <LITERAL_SCALAR_START: "|" ("-"|"+")? (<SPACE>)*> : BLOCK_INDENTED| <FOLDED_SCALAR_START: ">" ("-"|"+")? (<SPACE>)*> : BLOCK_INDENTED| <DASH: "-" ((<SPACE>)+ | ((<SPACE>)* <EOL>))> } <DEFAULT, FLOW> TOKEN :{  <#PAIR_SEPERATOR: ":" ((<SPACE>)+ | ((<SPACE>)* <EOL>))>| <SINGLE_QUOTED_KEY: <QUOTE_SINGLE> <STRING_SINGLE_BODY_FIRST> <QUOTE_SINGLE> (<SPACE>)* <PAIR_SEPERATOR>>| <DOUBLE_QUOTED_KEY: <QUOTE_DOUBLE> <STRING_DOUBLE_BODY_FIRST> <QUOTE_DOUBLE> (<SPACE>)* <PAIR_SEPERATOR>> }//# Plain (unquoted) scalars can't start with syntax chars, and can't contain//# colon+space.// read first line of the scalarTOKEN :{  // exclude ": " and " #"   <#EXCLUDED_SUBSTRINGS:  ~[":", "\r", "\n", "#"] | (":" ~[" ", "\r", "\n"]) | (~[" ", "\r", "\n"] ["#"])>| <PLAIN_SCALAR: <FIRST_CHARS> (<EXCLUDED_SUBSTRINGS>)*> 	{	 	  	if (matchedToken.image.startsWith("--- ") || matchedToken.image.startsWith("... ")) {	  		backup(matchedToken.image.length());	  		matchedToken.kind = FALSE_START;			matchedToken.image = "FALSE_START";				// we will need to unwind *all* indents			// for Document start and end indicators						// so force the column to 0			// tokenizer code with switch back to DEFAULT state			matchedToken.beginColumn = FIRST_COLUMN;	  		SwitchTo(DOCUMENT_START_END);	  	} else {	  		SwitchTo(INDENTED);	  	}	}| <PLAIN_KEY: <PLAIN_SCALAR> (<SPACE>)* <PAIR_SEPERATOR>> }// additional lines of plain scalar after the first are more forgiving.// we will take care to record the indent levels, style and chomp indicators// so we can properly represent the scalar later// all lines that have whitespace and newline only are sent to skeleton<INDENTED> TOKEN :{   <PLAIN_SCALAR_INDENTED: ~[" ", "\r", "\n", "#"] (<EXCLUDED_SUBSTRINGS>)*>	  {    // not a match if the indent is > than parent, put the token back    if (isIndentedScalarFinished(matchedToken)) {            	SwitchTo(DEFAULT);      	}  }}// all lines that have whitespace and newline only are sent to skeleton// comments only allowed after the key: > or key: | <BLOCK_INDENTED> TOKEN :{   <LITERAL_FOLDED_INDENTED: ~[" ", "\r", "\n"] (~["\r", "\n"])*>    {    // not a match if the indent is > than parent, put the token back    if (isIndentedScalarFinished(matchedToken)) {        	SwitchTo(DEFAULT);  	}  }}<FLOW> TOKEN :{   // read the first line of the flow scalar   // currently we don't allow wrapped plain scalars in flow context  <PLAIN_FLOW_SCALAR: <FIRST_CHARS> (~[",", "[", "]", "{", "}", ":", "?", "\n", "\r", "#"] | (~[" "] ["#"]))*>| <PLAIN_FLOW_KEY: <PLAIN_FLOW_SCALAR> (<SPACE>)* <PAIR_SEPERATOR>>  }<*> SPECIAL_TOKEN :{  <EOL : "\r\n" | ["\r", "\n", "\u0085", "\u2028", "\u2029"]> { ignorableWhiteSpace(matchedToken); }| <SPACE: [" ", "\u00a0", "\u1680", "\u180e", "\u2000"-"\u200b", "\u202f", "\u205f", "\u3000", "\ufeff"]> { ignorableWhiteSpace(matchedToken); }}//# A YAML Stream is the top level rule, and accounts for the entirety of the//# text being parsed. Basically, a stream is a set of zero or more documents,//# but there can be ignorable comments on either side of an explicitly marked//# document. NOTE: Not yet dealing with directives.void yaml_stream() : { Token v = null; }{  (v=<VERSION_TAG>)? {handler.handleOther(v!=null ? v.image : null);} (yaml_document())+}//# A YAML Document is a single node of any kind. It may start with an optional//# explicit head marker, and may be terminated with an optional explicit foot//# marker.void yaml_document() : { Token t = null; }{  (  	(document_start())?  	(LOOKAHEAD(2) node())+  	(LOOKAHEAD(2) document_end())?  )+  <EOF>}void document_start() :{ Token t = null; }{	// FALSE_START is case where PLAIN_SCALAR grabbed DOCUMENT_START and we had to backup the inputstream	(<FALSE_START>)? t=<DOCUMENT_START> { reset(); handler.handleDocumentStart(t.image); }}void document_end() :{ Token t = null; }{		// FALSE_START is case where PLAIN_SCALAR grabbed DOCUMENT_END and we had to backup the inputstream	(<FALSE_START>)? t=<DOCUMENT_END> { reset(); handler.handleDocumentEnd(t.image); }}void node() : { Token t; }{   (anchor_or_tag())*  ( sequence()    | mapping()      | scalar()    | t=<ALIAS> { handler.handleOther(t.image); }    | t=<TAG_DIRECTIVE> { handler.handleOther(t.image); }  )}void anchor_or_tag() : { Token t; }{  (t=<ANCHOR>|t=<TAG>) { handler.handleOther(t.image); }}void scalar() : { Scalar s; }{  (s = quoted_string()  | s = literal_style()   | s = folded_style()  | s = plain_scalar())  { handler.handleScalar(s); token_source.processIndentedBlockEnd();}   }void flow_scalar() : { Scalar s = null; Token t = null; }{  (s = quoted_string() | t = <PLAIN_FLOW_SCALAR>)  {  	if (s != null) {  		s.flow = true;  		{ handler.handleScalar(s);}   	} else if (t != null) {  		Scalar ts = new Scalar();  		ts.flow = true;  	  	ts.type = YamlScalarTypes.PLAIN;  	  	ts.scalar = t.image;  	  	{ handler.handleScalar(ts); }   	}  }}Scalar quoted_string() : { QuotedScalar q; }{      (q = double_quoted_string() | q = single_quoted_string())  {  	Scalar s = new Scalar();  	s.type = q.type;  	s.quoted = q;   	return s;   }}QuotedScalar double_quoted_string() : {  QuotedScalar q = new QuotedScalar();  q.type = YamlScalarTypes.DOUBLE;  Token f;  Token s;  Token e;   Token cs = null;  Token ce = null;}{     ((f=<DOUBLE_QUOTED_FIRST> (ce=<CONTINUATION_END>)?      {        // if there is a continuation char then preserve ending whitespace     	q.setFirstLine(f.image);     	if (ce != null) q.firstLineHasContinuation = true;      	ce = null;      }   ((cs=<CONTINUATION_START>)? s=<STRING_DOUBLE_BODY_SECOND> (ce=<CONTINUATION_END>)?      {      	// add possible skeleton;  		q.addLine(new Line(ignorableTextBefore(s, false)));  		// add indented line and indent column  		// if continuation characters at start or end  		// then preserve start or end whitespace  		Line ql = new Line(s.image, s.beginColumn, false);  		ql.setContinuation(cs == null ? false : true, ce == null ? false : true);  		q.addLine(ql);      }  )*   <QUOTE_DOUBLE_END>)  | e=<DOUBLE_QUOTED_EMPTY>      {    	q.setFirstLine("");      }  )    { return q; }  }QuotedScalar single_quoted_string() : {   QuotedScalar q = new QuotedScalar();  q.type = YamlScalarTypes.SINGLE;  Token f;  Token s;  Token e; }{   ((f=<SINGLE_QUOTED_FIRST>  (<CONTINUATION_END>)?     {		q.setFirstLine(f.image);          }   ((<CONTINUATION_START>)? s=<STRING_SINGLE_BODY_SECOND> (<CONTINUATION_END>)?      {      	// add possible skeleton;  		q.addLine(new Line(ignorableTextBefore(s, false)));  		// add indented line and indent column  		q.addLine(new Line(s.image, s.beginColumn, false));      }   )*  <QUOTE_SINGLE_END>)  | e=<SINGLE_QUOTED_EMPTY>    {    	q.setFirstLine("");    }  )    { return q; }  }Scalar plain_scalar() : { IndentedBlock indented; Token t = null; }{  // syntacticaly, indented_plain_scalar() is optional but we only determine  // this semantically (inside the Java code)  t = <PLAIN_SCALAR> indented = indented_plain_scalar()  {  	Scalar s = new Scalar();  	s.type = YamlScalarTypes.PLAIN;  	// trim in case a comment forced the lexer to eat some whitespace  	s.scalar = t.image.trim();  	s.indentedBlock = indented;  	return s;  }}Scalar literal_style() : { IndentedBlock indented; Token t; }{   t = <LITERAL_SCALAR_START> {handler.handleMarker(t.image);} indented = indented_block_scalar(YamlScalarTypes.LITERAL)   {      	Scalar s = new Scalar();  	s.type = YamlScalarTypes.LITERAL;  	s.indentedBlock = indented;  	s.setChomp(t.image);  	return s;  }}Scalar folded_style() : { IndentedBlock indented; Token t; }{  t = <FOLDED_SCALAR_START> {handler.handleMarker(t.image);} indented = indented_block_scalar(YamlScalarTypes.FOLDED)  {  	  	Scalar s = new Scalar();  	s.type = YamlScalarTypes.FOLDED;  	s.indentedBlock = indented;  	s.setChomp(t.image);  	return s;  }}IndentedBlock indented_plain_scalar() : { IndentedBlock ib = new IndentedBlock(); ib.type = YamlScalarTypes.PLAIN; Token t = null; }{  // NULL_SCALAR is used since we can't backup tokens. It's the result   // of a failed search for the next indented line  (t=<PLAIN_SCALAR_INDENTED>  	 {  		// add possible skeleton to TU, but not first newlines  		ib.addLine(new Line(ignorableTextBefore(t, false)));  		  		// add indented line and indent column  		ib.addLine(new Line(t.image, t.beginColumn, false));  		  	 }  )* (<NULL_SCALAR>)?    { if (ib.isEmpty()) return null; return ib; }}IndentedBlock indented_block_scalar(YamlScalarTypes type) :{ boolean firstSkeleton = true; IndentedBlock ib = new IndentedBlock(); ib.type = type; Token t = null; }{  // NULL_SCALAR is used since we can't backup tokens. It's the result   // of a failed search for the next indented line  (t=<LITERAL_FOLDED_INDENTED>  	 {  	 	// add possible skeleton to TU, but not first newlines  		ib.addLine(new Line(ignorableTextBefore(t, firstSkeleton)));  		firstSkeleton = false;  		  		// add indented line and indent column  		ib.addLine(new Line(t.image, t.beginColumn, false));   	 }	  )* (<NULL_SCALAR>)?  { if (ib.isEmpty()) return null; return ib; } }void flow_node() : { Token t;  }{	flow_sequence() | flow_mapping() | flow_scalar() | t=<ALIAS> { handler.handleOther(t.image); }}void sequence() : {}{  flow_sequence() | block_sequence()}void flow_sequence() : { Token t; }{  // allow for empty list  { handler.handleSequenceStart(true); }  <FLOW_SEQUENCE_START> (flow_sequence_entry() (t=<LIST_SEPERATOR>{ handler.handleMarker(t.image); })?)* <FLOW_SEQUENCE_END>  { handler.handleSequenceEnd(true); }}void flow_sequence_entry() : {}{  flow_node()}void indentless_block_sequence() : {}{    (block_sequence_element())+ }void block_sequence() : {}{  	  { handler.handleSequenceStart(false); }  <BLOCK_SEQUENCE_START> (block_sequence_element())+ <BLOCK_END>  { handler.handleSequenceEnd(false); } }void block_sequence_element() : { Token t; }{  t=<DASH> { handler.handleBlockSequenceNodeStart(t.image, t.beginColumn); } node() }Key key() : { Token t = null; Key k = new Key(); }{ 	  (t=<SINGLE_QUOTED_KEY> {k.key=t.image; k.type=YamlScalarTypes.SINGLE; }  | t=<DOUBLE_QUOTED_KEY> {k.key=t.image; k.type=YamlScalarTypes.DOUBLE; }     | t=<PLAIN_KEY> {k.key=t.image; k.type=YamlScalarTypes.PLAIN; })    { k.indent = t.beginColumn; return k; }   }Key flow_key() : { Token t = null; Key k = new Key(); k.flow = true; }{ 	  (t = <SINGLE_QUOTED_KEY> {k.key=t.image; k.type=YamlScalarTypes.SINGLE; }  | t= <DOUBLE_QUOTED_KEY> {k.key=t.image; k.type=YamlScalarTypes.DOUBLE; }   | t= <PLAIN_FLOW_KEY> {k.key=t.image; k.type=YamlScalarTypes.PLAIN; })     { return k; }    }void mapping() : {}{   flow_mapping() | block_mapping()}void flow_mapping() : { Token t; }{  // allow for empty map  { handler.handleMapStart(true); }  (<FLOW_MAPPING_START> (flow_mapping_element() (t=<LIST_SEPERATOR> { handler.handleMarker(t.image); })?)* <FLOW_MAPPING_END>)  { handler.handleMapEnd(true); }}void flow_mapping_element() : { Key k = null; }{    (k=flow_key() { handler.handleKey(k); } flow_node() {handler.handleMappingElementEnd();}) | flow_node()}void block_mapping() : {}{ { handler.handleMapStart(false); }  (<BLOCK_MAPPING_START> (block_mapping_element())+ <BLOCK_END>) { handler.handleMapEnd(false); } }void block_mapping_element() : { Key k; Token kl = null; }{  k=key() { handler.handleKey(k); } (node() | indentless_block_sequence())? {handler.handleMappingElementEnd();}}